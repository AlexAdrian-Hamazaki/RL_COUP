---
lesson_name: "2048_lesson2"
# Coup Lesson 2
# Train against random agent: 'random', weak opponent: 'weak', strong opponent: 'strong', or use self-play: 'self'
opponent: models/DQN/lesson1_2048_trained_agent.pt
opponent_pool_size: 6      # Size of opponent pool for self-play
evo_opp_epochs: 5      # Epoch frequency to update opponent pool

eval_opponent: random  # 'random', 'weak' or 'strong'
pretrained_path: models/DQN/lesson1_2048_trained_agent.pt  # Path to pretrained model weights
save_path: models/DQN/lesson2_2048_trained_agent.pt  # Path to save trained model

# HPO Parameters
epochs: 10
episodes_per_epoch: 5  # Number of games before updating Q network (number of episoder per epoch)
evo_epochs: 499 # Frequency of HPO evaluation and mutation
n_evaluations: 1000   # Number of evaluation episodes for hyperparameter selection
max_steps: 200 # Max steps in a game before it just resets. Required in case model cannot finish a game

# network parameters
epsilon: 1.0  # Starting epsilon value
eps_end: 0.05  # Final epsilon value
eps_decay: 0.9999  # Epsilon decay rate

# General things
env_name: 'COUP_v1.0'  # Environment name
algo: "DQN"  # Algorithm


# Warmup buffer stuff
buffer_warm_up: false # Fill replay buffer with experiences
warm_up_opponent: 'random' # Difficulty level of warm up experiences (usually random)
agent_warm_up: 0  # Number of epochs to warm up agent by training on random experiences

## Game specific:
n_players: 2
rewards:  # Rewards for different outcomes
    win: 1
    lose: -1
    coins: 0.1
    kill: 0.5
    lose_life: -1
    deck_knowledge: 0
    play_continues: 0